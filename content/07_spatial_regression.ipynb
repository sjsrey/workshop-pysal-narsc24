{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a20ebc0",
   "metadata": {},
   "source": [
    "# NARSC PySAL Workshop - Nov, 15 2023\n",
    "### Workshop by: Luc Anselin, Pedro Amaral\n",
    "#### Notebook adapted from Luc Anselin's course material "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c764aed",
   "metadata": {},
   "source": [
    "In this notebook, we cover a brief presentation of the main functionalities of the spreg library in PySAL. Spreg (short for spatial regression) supports the estimation of classic and spatial regression models. Currently, it contains methods for estimating standard and spatial versions of models such as Ordinary Least Squares (OLS), Two Stage Least Squares (2SLS), Seemingly Unrelated Regressions (SUR), and Random and Fixed effects panels. Additionally, it offers various tests of homoskedasticity, normality, spatial randomness, and different types of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48ad53",
   "metadata": {},
   "source": [
    "# 1. Basic Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f62c43",
   "metadata": {},
   "source": [
    "\n",
    "This section covers some elementary functionality to carry out data input and output as well as some basic data manipulations. The key concept is a so-called *DataFrame*, a tabular representation of the data with observations as rows and variables as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a0938",
   "metadata": {},
   "source": [
    "### Modules Needed\n",
    "\n",
    "This is implemented by means of *pandas* for generic text files (as well as many other formats) and *geopandas* for spatial data files (shape files).\n",
    "\n",
    "Before we can get to specific spatial functionality, we will be using *geopandas* to load data into so-called data frames. All of these rely on *numpy* as a dependency.\n",
    "\n",
    "Pandas and numpy are installed by default if you used anaconda. Geopandas may not be installed. If that is the case, use:\n",
    "\n",
    "`conda install -c conda-forge geopandas`\n",
    "\n",
    "Note: if you use pip install, it may not work properly, see https://geopandas.org/en/v0.4.0/install.html\n",
    "\n",
    "The full set of imports is shown below (note the special way to import geopandas due to some dependency issues - if you skip the os part, you may get a warning, but it still works).\n",
    "\n",
    "The work horse for spatial analysis in Python is the *PySAL* library. In particular we will be using the utilities in *libpysal* (needs to be imported separately) for spatial weights.\n",
    "\n",
    "It is good practice to check the version of libpysal and other PySAL modules to make sure the latest is being loaded.\n",
    "\n",
    "Finally, the main module for spatial regression in PySAL is *spreg*. This is the module that will be the focus of this workshop. Due to time constraints, we will not be able to review all of its functionalities, but the core one should be covered here.\n",
    "\n",
    "Should you want to check out *spreg*’s development, please visit https://github.com/pysal/spreg/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import os\n",
    "#os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "import libpysal\n",
    "print(libpysal.__version__)\n",
    "\n",
    "import spreg\n",
    "print(spreg.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3416ff",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "We will be using data on socio-economic correlates of health outcomes contained in the **Chi-SDOH** sample data set. We will be using more than one file, make sure they are in the working directory (the code assumes they are there, otherwise, you will need to change the code and specify the full path):\n",
    "\n",
    "- **Chi-SDOH.shp,shx,dbf,prj**: socio-economic indicators of health for 2014 in 791 Chicago tracts\n",
    "\n",
    "All the files are defined here, so that it will be easy for you to re-run the commands for your own applications. The only changes needed would be the file names and/or variable names (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data/Chicago-SDOH/\"\n",
    "infileshp = folder+\"Chi-SDOH.shp\"   # input shape file\n",
    "outfileshp =  folder+\"test.shp\"    # output shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc97ea",
   "metadata": {},
   "source": [
    "## Text Files\n",
    "\n",
    "### Input\n",
    "\n",
    "In Python, the easiest way to read shape files is to use *geopandas*. The command is `read_file`, followed by the file pathname in parentheses. The program is smart enough to figure out the file format from the file extension *.shp*. The result is a geopandas data frame, a so-called *GeoDataFrame*, say **dfs**, which is essentially a pandas data frame with an additional column for the geometry.\n",
    "\n",
    "All the standard pandas commands also apply to a geopandas data frame, such as `shape`.\n",
    "\n",
    "In the example, we use the **Chi-SDOH.shp** file as the input file, as specified in `infileshp` at the top of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd6bae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = gpd.read_file(infileshp)\n",
    "print(dfs.shape)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31293509",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "A quick summary of the data set is provided by the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936652f",
   "metadata": {},
   "source": [
    "### Extracting Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167db2c",
   "metadata": {},
   "source": [
    "Variables (columns) are extracted from a dataframe by listing their names in a list and subsetting the data frame. However, it is important to keep in mind that the result is a different view of the same data frame, which may not be what we want. In fact, in most applications, we will want the result to be a numpy array, so we need to make sure to cast the result as such.\n",
    "\n",
    "For example, we will extract the variables **EP_AGE17**, **Ovr6514P** and **Pop2014** to compute a dependency ratio, i.e., the ratio of population under 18 and over 65 to the active population (between 18 and 65). We will then add the result to the data frame.\n",
    "\n",
    "First, we put the variable names in a list to subset the data frame and check the type. Make sure to use double brackets, the argument to the subset [ ] is a list, so [[list]]. Note how the result is a data frame.\n",
    "\n",
    "Note: if you want to do this for your own data set, possibly using different variables and different expressions, you will need to adjust the code below accordingly. Typically, this is avoided in these notebooks, but here there is no option to make things totally generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef84f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfs[['EP_AGE17','Ovr6514P','Pop2014']]\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964daaa4",
   "metadata": {},
   "source": [
    "A more elegant approach and one that will make it much easier to reuse the code for different data sets and variables is to enter the variable names in a list first, and then pass that to subset the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = ['EP_AGE17','Ovr6514P','Pop2014','geometry']\n",
    "df1 = dfs[varnames]\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80a141",
   "metadata": {},
   "source": [
    "At this point, it is much more meaningful to get the descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddcd1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4508d",
   "metadata": {},
   "source": [
    "### Basic Choropleth Mapping\n",
    "\n",
    "Geopandas contains some basic mapping functionality through the `plot` function. This is usually fine for rudimentary maps, but several other packages are available that contain greater functionality. For our purposes, the `plot` functionality is sufficient.\n",
    "\n",
    "We start with a basic choropleth map of the **Pop2014** variable.\n",
    "\n",
    "We need to specify the variable to be mapped, the size of the figure and a legend. Details are beyond the current scope, but the examples below provide some templates.\n",
    "\n",
    "A preferred way of dealing with this is to export the spatial data frame to a new shape file and use that for further exploration in **GeoDa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.plot('Pop2014', figsize=(6, 6),legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699bafa",
   "metadata": {},
   "source": [
    "Not exactly the prettiest thing in the world. As usual, there are many options, too many to cover here. The most important ones are `cmap`, a colormap (defined in matplotlib) defined by a code, `scheme`, a map classification, and some arguments to the legend to fine tune its position. The latter are passed as a `legend_kws` dictionary. Here, standard quartiles are used, but for more fine tuned classifications, the the *mapclassify* module of PySAL should be imported.\n",
    "\n",
    "To change the map from the default to a quartile map using an brownish color ramp and moving the legend to the right is illustrated below. Note that the ranking of the color ramp is the reverse from before, with lighter colors corresponding to lower dependency rates. Darker colors indicate more children and/or elderly.\n",
    "\n",
    "For detailed information on the full range of options, see the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.plot('Pop2014',figsize = (6, 6), cmap='OrRd',scheme='quantiles',k=4,legend=True,\n",
    "       legend_kwds={'loc':'center left','bbox_to_anchor':(1,0.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0598e91",
   "metadata": {},
   "source": [
    "### Writing a Shape File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7c818",
   "metadata": {},
   "source": [
    "As mentioned, a preferred approach is to export the spatial data frame and do the visualization in **GeoDa**.\n",
    "\n",
    "The output is accomplished by the `to_file` command. This supports many different output formats, but the default is the ESRI shape file, so we do not have to specify any arguments other than the filename. Here, we use the output file name specified in `outfileshp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_file(outfileshp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad019ff",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca1664",
   "metadata": {},
   "source": [
    "Use your own data set or one of the GeoDaCenter sample data sets to load a spatial data frame, create some new variables, optionally get descriptive statistics or map the data. We will be using this type of operation frequently in the course of the regression analysis, when we will add predicted values and/or residuals to a map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7f126",
   "metadata": {},
   "source": [
    "# 2. Spatial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caaee5e",
   "metadata": {},
   "source": [
    "\n",
    "This section covers a brief review of some basic operations pertaining to spatial weights: how to read them, construct them, and use them to create spatially lagged variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da216d",
   "metadata": {},
   "source": [
    "### Files and Variables\n",
    "\n",
    "We will again be using data on socio-economic correlates of health outcomes contained in the **Chi_SDOH** sample shape files and associated spatial weights. The addition now are:\n",
    "\n",
    "- **Chi-SDOH_q.gal**: queen contiguity spatial weights from `GeoDa`\n",
    "- **Chi-SDOH_k6s.gal**: k-nearest neighbor weights for k=6, made symmetric in `GeoDa`\n",
    "\n",
    "As before, we specify file names and variable names at the top of the notebook so that this is the only part that needs to be changed if you want to apply the code to your own data sets and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a910c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "infileq = folder+\"Chi-SDOH_q.gal\"           # queen contiguity from GeoDa\n",
    "infileknn = folder+\"Chi-SDOH_k6s.gal\"       # symmetric k-nearest neighbor weights from GeoDa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684272c8",
   "metadata": {},
   "source": [
    "Output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfileq = folder+\"test_q.gal\"            # output file for queen weights computed with libpysal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9fb6b",
   "metadata": {},
   "source": [
    "To illustrate spatially lagged variables, we will use the variable **YPLL_rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47062027",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = [\"YPLL_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051ceb7",
   "metadata": {},
   "source": [
    "## Spatial Weights from GeoDa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51160fd3",
   "metadata": {},
   "source": [
    "Spatial weights are an essential part of any spatial autocorrelation analysis or implementation of spatial regression. Functionality to create and analyze spatial weights is contained in the `libpysal` library.\n",
    "The full range of functions is much beyond our scope and you are referred to the documentation and API for further details. Only the essentials are covered here, sufficient to proceed\n",
    "with the spatial regression analysis.\n",
    "\n",
    "Arguably the easiest way to create spatial weights is to use `GeoDa`, which\n",
    "provides functionality to construct a wide range of contiguity as well as distance\n",
    "based weights through a graphical user interface. The weights information is stored as **gal**, **gwt** or **kwt** files. We will consider this first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6ef20",
   "metadata": {},
   "source": [
    "### Queen Contiguity Weights\n",
    "\n",
    "Contiguity weights can be read into PySAL spatial weights objects using the `read` function, after opening the file with `libpysal.io.open`. We apply this to the queen contiguity created by `GeoDa`, contained in **infileq**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce630240",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = libpysal.io.open(infileq).read()\n",
    "wq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519f624",
   "metadata": {},
   "source": [
    "The result is a PySAL spatial weights object of the class `libpysal.weights.weights.W`. In essence, this is a dictionary, with as keys `neighbors` and `weights`. Weights objects have several attributes, which can be accessed using the familiar dot notation. Some of the more important ones are illustrated below.\n",
    "\n",
    "First, to illustrate the contents of the weights object, we consider the neighbors and weights for the first observation. \n",
    "These items are identical to the neighbors list for the first observation in the GAL file.\n",
    "\n",
    "Remember that these are dictionaries, so you have to provide the proper *key* to access their elements. For example, for the first observations, this is **'1'** (with the quotes, hence a character) and not **0**, as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.neighbors['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wq.neighbors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53fcb0",
   "metadata": {},
   "source": [
    "More explicitly, the object **wq.neighbors** is a dictionary. Its *keys* can be extracted by means of `wq.neighbors.keys()`. However, this will give all 791 values. In order to get a shorter list, we first turn the keys into a `list` and then `print` just the first five elements. It's always a good idea to check this if you are unsure of the values of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a344dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(wq.neighbors.keys())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d3b5c",
   "metadata": {},
   "source": [
    "The weights associated with each observation key are found using `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.weights['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee4c2d",
   "metadata": {},
   "source": [
    "A quick check on the number of observations, i.e., the number of rows in the weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60409f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wq.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751a0a5",
   "metadata": {},
   "source": [
    "Minimum, maximum and average number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de46f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wq.min_neighbors,wq.max_neighbors,wq.mean_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8f729",
   "metadata": {},
   "source": [
    "Percent non-zero neighbors (sparseness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bf7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wq.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406929f",
   "metadata": {},
   "source": [
    "There is no explicit check for symmetry as such, but instead the lack of symmetry can be assessed by means of the `asymmetry` method, or the list of id pairs with asymmetric weights is obtained by means of the `asymmetries` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.asymmetry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5282bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.asymmetries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d5999",
   "metadata": {},
   "source": [
    "Since contiguity weights are symmetric by construction, the presence of an asymmetry would indicate some kind of error. This is not the case in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ee8a3",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856073b",
   "metadata": {},
   "source": [
    "Similarly, we can read in the symmetric knn weights (k=6) created by `GeoDa` from the file **infileknn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f85ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wk6s = libpysal.io.open(infileknn).read()\n",
    "wk6s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea8c6c",
   "metadata": {},
   "source": [
    "Some characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c91c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6s.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fc305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wk6s.min_neighbors,wk6s.max_neighbors,wk6s.mean_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cd898",
   "metadata": {},
   "source": [
    "Note how the operation to make the initially asymmetric k-nearest neighbor weights symmetric has resulted in many observations having more than 6 neighbors. That is the price to pay to end up with symmetric weights, which is required for some of the estimation methods. We can list neighbors and weights in the usual way. As it turns out, the observation with key `1` is not adjusted, but observation with key `3` now has eight neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb35473",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6s.neighbors['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5d207",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wk6s.neighbors['3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc8b5b",
   "metadata": {},
   "source": [
    "## Weights from a GeoDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193a5dd",
   "metadata": {},
   "source": [
    "### Queen Contiguity Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713be4b9",
   "metadata": {},
   "source": [
    "The weights construction is handled by the `libpysal` package in PySAL. The function is `weights.Queen.from_dataframe` with as arguments the geodataframe and optionally the `ids` (recommended). For the Chicago data, the ID variable is **OJECTID**. That is why we made sure it was turned into an integer first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1 = libpysal.weights.Queen.from_dataframe(dfs,ids='OBJECTID')\n",
    "wq1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a443",
   "metadata": {},
   "source": [
    "A quick check on the keys reveals these are indeed integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e404c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(wq1.neighbors.keys())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9263cc3",
   "metadata": {},
   "source": [
    "Again, some characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbad937",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.min_neighbors,wq1.max_neighbors,wq1.mean_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf427f5",
   "metadata": {},
   "source": [
    "The structure of the weights is identical to that from the file read from `GeoDa`. For example, the first set of neighbors is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7321592",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.neighbors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a008d00",
   "metadata": {},
   "source": [
    "With associated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca59fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc46a3",
   "metadata": {},
   "source": [
    "### Row-standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756cb5e",
   "metadata": {},
   "source": [
    "As created, the weights are simply 1.0 for binary weights. To turn the weights into row-standardized form, a *transformation* is needed, `wq1.transform = 'r'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.transform = 'r'\n",
    "wq1.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659247c",
   "metadata": {},
   "source": [
    "### Writing a Weights File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e387f1",
   "metadata": {},
   "source": [
    "To write out the weights object to a GAL file, we have to use `libpysal.io.open`, however this time with the `write` method. The argument to the `open` command is the filename and `mode='w'` (for writing a file). The weights object itself is the argument to the `write` method.\n",
    "\n",
    "Note that even though the weights are row-standardized, this information is lost in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0547d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "libpysal.io.open(outfileq,mode='w').write(wq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c928df1",
   "metadata": {},
   "source": [
    "A quick check using the `weights.Queen.from_file` operation on the just created weights file.\n",
    "\n",
    "We also check the dimension and the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ef494",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1a = libpysal.weights.Queen.from_file(outfileq)\n",
    "print(wq1a.n)\n",
    "print(list(wq1a.neighbors.keys())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26a5b6",
   "metadata": {},
   "source": [
    "Note how the type of the key has changed from integer above to character after reading from the outside file. This again stresses the importance of checking the keys before any further operations.\n",
    "\n",
    "The weights are back to their original binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e60e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wq1a.weights['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e53bca",
   "metadata": {},
   "source": [
    "### KNN Weights\n",
    "\n",
    "The corresponding functionality for k-nearest neighbor weights is `weights.KNN.from_dataframe`. An important argument is `k`, the number of neighbors, with the default set to `2`, which is typically not that useful. Again, we include OBJECTID as the ID variable. Initially the weights are in binary form and we carry out a row-standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6 = libpysal.weights.KNN.from_dataframe(dfs,k=6,ids='OBJECTID')\n",
    "wk6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce55ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wk6.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab989b",
   "metadata": {},
   "source": [
    "A quick check on the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(wk6.neighbors.keys())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5173eb",
   "metadata": {},
   "source": [
    "We can check the neighbors of observation 3. This is a subset of six from the list of eight from the above symmetric knn weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599541a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6.neighbors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4f435",
   "metadata": {},
   "source": [
    "The k-nearest neighbor weights are intrinsically asymmetric. Rather than listing all the pairs that contain such asymmetries, we can check on the length of this list using the `asymmetry` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b112430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(wk6.asymmetry()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aec84c",
   "metadata": {},
   "source": [
    "KNN weights have a built-in method to make them symmetric: `symmetrize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6s2 = wk6.symmetrize()\n",
    "print(len(wk6.asymmetry()))\n",
    "print(len(wk6s2.asymmetry()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55eca5",
   "metadata": {},
   "source": [
    "The entries are now the same as for the symmetric knn GAL file that was read in from `GeoDa`. For example, the neighbors of observation with key `3` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5eb99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wk6s2.neighbors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c828bbe",
   "metadata": {},
   "source": [
    "Finally, to make them row-standardized, we carry out the transformation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk6s2.transform = 'r'\n",
    "wk6s2.weights[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f215b48",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Experiment with various spatial weights for your own data set or for one of the GeoDa sample data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a7247",
   "metadata": {},
   "source": [
    "# 3. Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868da366",
   "metadata": {},
   "source": [
    "This section covers a brief review of the basic OLS regression and elementary regression diagnostics. In addition, we introduce Moran's I as a first test for spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbe12b",
   "metadata": {},
   "source": [
    "We will illustrate the various regression models in the context of the *immigrant paradox* and use\n",
    "four variables from the SDOH data set: **YPLL_rate** (an index measuring premature mortality, i.e., higher values are worse health outcomes), **HIS_ct** (economic hardship index), **Blk14P** (percent Black population), and **Hisp14P** \n",
    "(percent Hispanic population).\n",
    "\n",
    "We first create lists with the variable names for the dependent variable (y_name), the explanatory variables (x_names1 and x_names2), the data set (optional, as ds_name), and the weights (optional, as w_name). Note that the dependent variable needs to be a string, not a list with one element. On the other hand, the ds_name and w_name can be either a string or a list with one string element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'YPLL_rate'\n",
    "x_names1 = ['Blk14P','Hisp14P']\n",
    "x_names2 = ['Blk14P','Hisp14P','HIS_ct']\n",
    "ds_name = 'Chi-SDOH'\n",
    "w_name = 'Chi-SDOH_q'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310f464",
   "metadata": {},
   "source": [
    "#### Setting up the variables:\n",
    "*spreg* will automatically identify single dimension arrays and convert them to two dimensions with a single column. But it is always a good practice to work with correct dimention arrays, as we do with the y vector in this example.\n",
    "\n",
    "Note that we do not include a constant term in the x matrices. This is included by default in the `spreg.OLS` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9122466",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs[y_name].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a24703",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs[y_name].values.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = dfs[x_names1].values\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07727ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = dfs[x_names2].values\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b17262",
   "metadata": {},
   "source": [
    "## OLS Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf37d1b",
   "metadata": {},
   "source": [
    "We start with the simplest of OLS regressions where only y and X are specified as arguments in the `spreg.OLS` command.\n",
    "\n",
    "The resulting OLS object has many attributes (see the docs for full details). An easy (the easiest) way to list the results is to print the `summary` attribute.\n",
    "\n",
    "First, the regression with the two ethnicity explanatory variables. We set the option `nonspat_diag=False`, since the default will give us the diagnostics, to which we return below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1 = spreg.OLS(y, x1, nonspat_diag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d40af",
   "metadata": {},
   "source": [
    "The basic result of this operation is to create an OLS object. We can see the range of attributes and methods this includes by means of the `dir` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d10e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(ols1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bc1da",
   "metadata": {},
   "source": [
    "Arguably, the most useful of these is the `summary` method, which provides a nice looking output listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69dccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731d364",
   "metadata": {},
   "source": [
    "As is, the output does not list any variable names or other information on the data set. In order to have a more informative output, `name_y`, `name_x` and `name_ds` should be specified in the arguments, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1a = spreg.OLS(y, x1, nonspat_diag=False,\n",
    "                  name_y=y_name, name_x=x_names1, name_ds=ds_name)\n",
    "print(ols1a.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00aa4ab",
   "metadata": {},
   "source": [
    "This bare bones regression output lists the variables and data set involved, and, in addition to the coefficient estimates, standard errors, t-statistics and p-values, includes the $R^2$ and adjusted $R^2$ as measures of fit. The mean and standard deviation of the dependent variable are given as well. These values are the same as computed in the previous notebook.\n",
    "\n",
    "The overall fit of about 0.60 is reasonable and both slope coefficients are positive and highly significant. However, this result is somewhat misleading, which is revealed when the economic hardship variable is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2 = spreg.OLS(y, x2, nonspat_diag=False,\n",
    "                 name_y=y_name,name_x=x_names2,name_ds=ds_name)\n",
    "print(ols2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3beb65b",
   "metadata": {},
   "source": [
    "The inclusion of economic hardship turned the coefficient of the Hispanic share from positive to\n",
    "negative. This is the so-called *immigrant paradox*. All coefficients are highly significant, with an adjusted $R^2$ of 0.6316."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78c8bc",
   "metadata": {},
   "source": [
    "### Predicted Values and Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36bc6c",
   "metadata": {},
   "source": [
    "Two important attributes of the regression object are the predicted values and residuals. Unlike many of the other attributes, these are full-length column vectors of size n x 1. They can be extracted as the `predy` and `u` attributes. \n",
    "\n",
    "The most useful application of the predicted values and residuals in a diagnostic sense is to plot and map them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(np.hstack((ols2.predy,ols2.u)),columns=['ypred','resid'])\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfs,preds],axis=1).plot('resid', figsize=(6, 6),cmap='OrRd',scheme='quantiles',k=4,legend=True,\n",
    "       legend_kwds={'loc':'center left','bbox_to_anchor':(1,0.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf3d3e",
   "metadata": {},
   "source": [
    "## Non-Spatial Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac5682",
   "metadata": {},
   "source": [
    "The default setting for OLS regression is to always include the non-spatial diagnostics, with `nonspat_diag=True`. Since this is the default, this argument does not have to be set. We repeat the second regression with the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2a = spreg.OLS(y,x2,\n",
    "                 name_y=y_name,name_x=x_names2,name_ds=ds_name)\n",
    "print(ols2a.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1730b",
   "metadata": {},
   "source": [
    "The previous output is now agumented with several additional measures of fit (listed above the coefficients) as well as diagnostics for multicollinearity, non-normality and heteroskedasticity (listed below the coefficients).\n",
    "\n",
    "The measures of fit on the left-hand column are all related to the sum of squared residuals. This is listed, as well as two measures of $\\sigma^2$ (one from division by the degrees of freedom, the other - ML - from division by the number of observations) and their square roots, the S.E. of regression.\n",
    "\n",
    "On the right-hand side are the results of an F-statistic for the joint significance of the coefficients and its associated p-value, the log-likelihood (under the assumption of normality) for use in comparisions with spatial models, and two adjustments of the log-likelihood for the number of variables in the model, the AIC and SC. Whereas a better fit is reflected in a higher log-likelihood, it is the reverse for AIC and SC (lower is better).\n",
    "\n",
    "Below the listing of coefficients are the multicollinearity condition number, the Jarque-Bera test on normality of the errors and two tests for heteroskedasticity (random coefficients): the Breusch-Pagan LM test and the more robust (against non-normality) Koenker-Bassett test.\n",
    "\n",
    "There is no evidence of a problem with multicollinearity, but a strong indication of both non-normality and heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0617e69",
   "metadata": {},
   "source": [
    "### White test\n",
    "\n",
    "Because it requires additional computation, the White test against heteroskedasticity is not included by default. To include it, the argument `white_test=True` must be set.\n",
    "\n",
    "All the output is the same as before, except for the addition of the test results, which again strongly indicate the presence of heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12992479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2b = spreg.OLS(y,x2,white_test=True,\n",
    "                 name_y=y_name,name_x=x_names2,name_ds=ds_name)\n",
    "print(ols2b.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552eb4dd",
   "metadata": {},
   "source": [
    "## Spatial Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd339ea3",
   "metadata": {},
   "source": [
    "Moran's I is a very powerful misspecification test. While designed to detect spatial error autocorrelation, it also has power against a range of other misspecifications, including heteroskedasticity and non-normality. Hence, when the null is *not* rejected, one can be fairly confident that none of these misspecifications are present. On the other hand, when the null is rejected, it is not always clear what the next step should be, other than maybe implement HAC standard errors. More focused tests against spatial autocorrelation are considered in the notebook that deals with Maximum Likelihood estimation.\n",
    "\n",
    "Moran's I test is invoked by setting the argument `moran=True`. However, this only works when `spat_diag=True`, which also gives all the other diagnostics for spatial dependence. At this point, we will ignored those. Without `spat_diag=True`, the standard output is obtained, without any diagnostics for spatial effects.\n",
    "\n",
    "Of course, in order to get spatial diagnostics, a spatial weights object `w` must be specified, with its name optionally as `name_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba5fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols5 = spreg.OLS(y,x2,spat_diag=True,moran=True,\n",
    "                 w=wq1,name_w=w_name,\n",
    "                 name_y=y_name,name_x=x_names2,name_ds=ds_name)\n",
    "print(ols5.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca542e1e",
   "metadata": {},
   "source": [
    "The output is again the same as before, with the addition of Moran's I as the first item under the heading *Diagnostics for Spatial Dependence*. The statistic is 0.0483, with a matching z-value of 2.619, which is significant at 1% level, showing evidence of spatial effects.\n",
    "\n",
    "However, the Moran's I is not very informative as to what to do from here. Given the results for the spatial diagnostics, which spatial regression model should be considered as the alternative? Common diagnostics to assess the presence of spatial dependence include a collection of Lagrange Multiplier test statistics.\n",
    "\n",
    "Historically, a forward specification search based on the LM statistics from an OLS regression was preferred in order to avoid the computational complexities associated with estimating the spatial models. This is not really an issue anymore, especially with the IV/GMM methodology and a backward specification search is a viable alternative.\n",
    "\n",
    "Also, the use of the asymptotic t-test instead of the LM tests is a viable alternative.\n",
    "\n",
    "The rationale behind the forward specification search is simple:\n",
    "\n",
    "- step 1: assess the significance of the LM tests\n",
    "  - if one is significant, this is the alternative model\n",
    "  - if both are significant go to step 2\n",
    "- step 2: assess the significance of the robust LM tests\n",
    "  - if one is significant, this is the alternative model\n",
    "  - if both are significant, go with the one with the highest value\n",
    "  - if neither is significant, go with the one with the highest value\n",
    "  \n",
    "In any case, once the spatial model is estimated, the asymptotic t-tests are available as a further check on the validity of the model.\n",
    "\n",
    "The forward specification decision rule is depicted in Figure 5.1 (p. 110) of Anselin and Rey (2004) (borrowed from Anselin, 2005).\n",
    "\n",
    "From our test results, we see that Lagrange Multiplier (lag) statistic is significant at 1% level, hence rejecting the null hypothesis of a standard linear regression specification with spatial randomness in favor of a spatial lag specification. However, the Lagrange Multiplier (error) statistic is also significant at 1% level, suggesting a spatial error specification. Since both statistics reject their null, we need to consider their robust versions. \n",
    "\n",
    "It is important to keep in mind that the robust Lagrange Multiplier statistics should only be considered when both LM (lag) and LM (error) reject the null, but never when one of these statistics (or both) does not reject the null.\n",
    "\n",
    "In this example, neither Robust LM (lag) or Robust LM (error) statistics are significant at 5%. In cases where both robust test statistics may still be signiﬁcant, the preferred alternative would be the one with the highest value for the statistic. Alternatively, we can test both specifications and choose the one with best model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc17f78",
   "metadata": {},
   "source": [
    "<img src=\"figs/flowchart.png\" alt=\"Spatial specification search chart\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aadbff",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "At this point, it would be useful to set up your own baseline regression model, with a continuous dependent variable and at least two or three explanatory variables. You can pick any set of variables from the Chicago data set, from one of the GeoDa sample data sets or your own data, but of course, makes sure that your research question makes sense. Create some different types of spatial weights, such as contiguity and KNN weights for the Moran's I test.\n",
    "\n",
    "Assess the extent to which your initial specification may suffer from some forms of misspecification, as well as the sensitivity of your results to different measures of standard errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df294694",
   "metadata": {},
   "source": [
    "# 4. IV/GMM Estimation of Spatial Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc29d5",
   "metadata": {},
   "source": [
    "This section we consider estimation of linear regression models based on instrumental variables (IV) and\n",
    "the general method of moments (GMM). Models covered are the spatial lag model, the spatial error model, the spatial Durbin model, as well as the higher order models. Direct and indirect effects are illustrated as well. Models that include additional endogenous variables are briefly introduced.\n",
    "\n",
    "Implementation of the GMM methods is included in a series of `PySAL-spreg` functions prefaced with `GM` and `GMM`, specifically `spreg.GM_Lag` and `spreg.GMM_error`. Beyond the classic lag and error models, additional specifications are set by means of the arguments `slx_lags` and `add_wy` for, respectively, spatially lagged regressors (WX) and a spatially lagged dependent variable (Wy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a202b4c",
   "metadata": {},
   "source": [
    "We will illustrate the various regression models in the context of the *immigrant paradox* and use\n",
    "four variables from the SDOH data set: **YPLL_rate** (an index measuring premature mortality, i.e., higher values are worse health outcomes), **HIS_ct** (economic hardship index), **Blk14P** (percent Black population), and **Hisp14P** \n",
    "(percent Hispanic population). In most specifications, we will treat the regressors as exogenous. We will illustrate the incorporation of additional endogenous variables by considering **HIS_ct** to be endogenous, with associated instruments **C_X** and **C_Y** (the census tract centroids).\n",
    "\n",
    "We first create lists with the variable names for the dependent variable (y_name), the explanatory variables (x_names and xe_names), the endogenous variable (yend_names) and instruments (q_names), the data set (optional, as ds_name), the weights (optional, as w_name) and kernel weights (optional, as gwk_name). Note that the dependent variable needs to be a string, not a list with one element. On the other hand, the ds_name and w_name can be either a string or a list with one string element. The other variable names need to be specified as lists, even when there is only one, as is the case here for `yend_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'YPLL_rate'\n",
    "x_names = ['Blk14P','Hisp14P','HIS_ct']\n",
    "xe_names = ['Blk14P','Hisp14P']\n",
    "yend_names = ['HIS_ct']\n",
    "q_names = ['C_X', 'C_Y']\n",
    "ds_name = 'Chi-SDOH'\n",
    "w_name = 'Chi-SDOH_q'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e25538",
   "metadata": {},
   "source": [
    "### Reading the Shape File\n",
    "\n",
    "Since we are already very ar from the beginning of this notebook, let us review and reload the original data using the `read_file` command from *geopandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = gpd.read_file(infileshp)\n",
    "print(dfs.shape)\n",
    "print(list(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18491090",
   "metadata": {},
   "source": [
    "Finally, just to make sure the indicator variable OBJECTID is integer, we explicitly set its type to integer. We may need this later for the weights manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abeb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.astype({'OBJECTID':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b8116",
   "metadata": {},
   "source": [
    "### Setting up the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0598e",
   "metadata": {},
   "source": [
    "Note that we do not include a constant term in the x matrices. This is included by default in the `spreg` estimation routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbaab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs[y_name].values.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40524b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfs[x_names].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = dfs[xe_names].values\n",
    "xe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yend = dfs[yend_names].values.reshape(-1,1)\n",
    "yend.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5230a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dfs[q_names].values\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be52a2b",
   "metadata": {},
   "source": [
    "### Reading the Spatial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53266a9a",
   "metadata": {},
   "source": [
    "To keep life simple, we will read the weights from the files created with `GeoDa`. After reading the queen contiguity weights, we row-standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = libpysal.io.open(infileq).read()\n",
    "print(wq.n)\n",
    "wq.transform = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da10ff5",
   "metadata": {},
   "source": [
    "## Spatial Lag Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d858e",
   "metadata": {},
   "source": [
    "IV/GMM estimation of the spatial lag model is carried out by means of `spreg.GM_Lag`. This is a customized implementation of the two stage least squares estimation where the instruments for the spatially lagged dependent variable are computed internally. Additional endogenous variables can be specified as well.\n",
    "\n",
    "The default setup requires the dependent variable, `y`, the explanatory variables, `x`, and the spatial weights `w`. The instruments are the first order spatially lagged explanatory variables, WX. They do not need to be specified separately. The order of spatial weights used as instruments is set by means of `w_lags` (the default is `w_lags = 1`).\n",
    "\n",
    "The A-K test results for remaining error spatial autocorrelation are included by specifying `spat_diag=True` as one of the arguments.\n",
    "\n",
    "As before, we will list the main results using the `summary` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369bc7e",
   "metadata": {},
   "source": [
    "### Exogenous Variables Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1 = spreg.GM_Lag(y,x,w=wq,spat_diag=True,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(lag1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ded5c6",
   "metadata": {},
   "source": [
    "The regression output follows the same format as for OLS, with the addition of the **Instrumented** or endogenous variables, in this case the spatially lagged dependent variable, and the **Instruments**, the spatially lagged regressors. Note that there are no likelihood-based measures of fit. Also, the R-squared is not a *real* R-squared, but the squared correlation between the predicted values and the observed dependent variable. There are two variants: the first is based on the *naive* predicted values (ignoring the endogeneity of Wy), the second on the use of the predicted value from the reduced form.\n",
    "\n",
    "In this specification, the spatial autoregressive coefficient is 0.165, but not significant at \n",
    "p < 0.07. The A-K test does not suggest the presence of any remaining error spatial autocorrelation.\n",
    "\n",
    "As was the case for OLS regression, the lag regression object `lag1` contains many attributes. Here, we will only use the `summary` method, but as before, predicted values, residuals and regression coefficients can be extracted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254db2c4",
   "metadata": {},
   "source": [
    "The default IV estimation is to use only the first order spatially lagged explanatory variables as instruments. To have the second order included as well requires the option `w_lags=2`, which we will use in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41310c4",
   "metadata": {},
   "source": [
    "### Heteroskedastic-Robust Standard Errors\n",
    "\n",
    "The default result for the spatial lag model is to provide the classic estimators for standard errors. Heteroskedastic-robust standard errors are obtained in the usual fashion with the `robust=\"white\"` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013ebb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lag2 = spreg.GM_Lag(y,x,w=wq,w_lags=2,robust=\"white\",\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(lag2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8b539",
   "metadata": {},
   "source": [
    "Compared to the default results, the estimates are not affected, but the standard errors are.\n",
    "Most importantly, the spatial autoregressive coefficient is no longer significant, with the standard errors roughly doubled and p=0.34."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ff0f1",
   "metadata": {},
   "source": [
    "### Exogenous and Endogenous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb788bc",
   "metadata": {},
   "source": [
    "To illustrate the inclusion of additional endogenous variables (beyond the spatially lagged dependent variable), we use the same `spreg.GM_Lag` command, but include `yend` for the endogenous variables and `q` for the associated instruments in the argument list. Optionally, `name_yend` and `name_q` can be specified as well. The proper matrix of exogenous regressors is `xe`. Everything else is the same as for the standard lag specification.\n",
    "\n",
    "Note that the example here is purely to illustrate the functionality and should not be interpreted in substantive terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag3 = spreg.GM_Lag(y,xe,yend=yend,q=q,w=wq,w_lags=2,\n",
    "                   name_yend=yend_names,name_q=q_names,\n",
    "                   name_y=y_name,name_x=xe_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(lag3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afa730",
   "metadata": {},
   "source": [
    "The main impact of including additional endogeneity is on the spatial autoregressive coefficient, which becomes essentially negligible. It remains to be seen whether the initial indication of a spatial lag effect was due to a real spatial process or instead due to ignoring endogeneity. In the current context, this is hard to establish.\n",
    "\n",
    "In what follows, we will not consider this case, but it works the same everywhere. Additional endogenous variables are specified as `yend` with instruments `q`. These are the only necessary addition to the arguments.\n",
    "\n",
    "Also, as in all cases, robust standard errors are obtained with `robust='white'` or `robust='hac'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f05bbe",
   "metadata": {},
   "source": [
    "### Multipliers\n",
    "\n",
    "In models that include a spatially lagged dependent variable and/or spatially lagged explanatory variables, the\n",
    "impact of a change in X on y is not simply the coefficient of X, as is the case in the standard regression model. Instead, the effect that results from changes in the neighboring values must also be accounted for. \n",
    "\n",
    "In Kim, Phipps and Anselin (2003), it was shown that if the change in the explanatory variable is uniform across observations, the *spatial multiplier* amounts to $1 / (1- \\rho)$, with the total effect of a change in variable\n",
    "$x_k$ amounting to $\\beta_k / (1 - \\rho)$. In our example, the spatial multiplier in the second spatial lag model (**lag2**) would be \n",
    "1.0 / (1.0 - 0.17663) = 1.215.\n",
    "\n",
    "The Kim et al. approach then distinguished between the direct effect, i.e., the coefficient of the $\\beta$ coefficients as estimated, and the total effect, which corresponds to this coefficient times the multiplier. An indirect effect is then the difference between the two.\n",
    "\n",
    "LeSage and Pace (2009) introduce a slightly different set of concepts and use the terms average direct impact (ADI), average indirect impact (AII) and average total impact (ATI) as summaries computed from the matrix expression $(I - \\rho W)^{-1}$ from the reduced form, $(I - \\rho W)^{-1}X\\beta$.\n",
    "\n",
    "There is currently no functionality in official release of `spreg` for the computation of impacts. The direct, indirect and total impacts are simply the coefficients in the model multiplied by the respective multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3be03",
   "metadata": {},
   "source": [
    "### Kim, Phipps and Anselin (2003) Interpretation\n",
    "\n",
    "The spatial multiplier, with the estimate for $\\rho$ as the **last** element in the `betas` attribute, is easy to compute. It is also stored in thr regression object as the attribute `rho`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92dca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag2.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "ati0 = 1.0 / (1 - lag2.rho)\n",
    "print(ati0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911dd2c",
   "metadata": {},
   "source": [
    "The direct, indirect and total effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f78d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdir0 = lag2.betas[1:-1]\n",
    "print(\"Direct effect:\\n\", bdir0)\n",
    "print(\"Total effect:\\n\", ati0*bdir0)\n",
    "print(\"Indirect effect:\\n\", ati0*bdir0 - bdir0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877b796",
   "metadata": {},
   "source": [
    "## Spatial Durbin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cfa2e",
   "metadata": {},
   "source": [
    "In essence, the Spatial Durbin model is a special case of a spatial lag model with spatially lagged regressors (WX) included. For the estimation of the spatial autoregressive coefficients, this means that the usual WX terms cannot be used as instruments, but higher order lags are needed, to avoid multicollinearity. This is handled internally.\n",
    "\n",
    "The Spatial Durbin is specified by including the `slx_lags` term as one of the arguments. The default is `slx_lags = 1`, but higher orders are possible as well.\n",
    "\n",
    "The other arguments are the same as for the standard `spreg.GM_Lag` command. The output listing indicates that the results are for a Spatial Durbin model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spdur = spreg.GM_Lag(y,x,w=wq,w_lags=2,slx_lags=1,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(spdur.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef1cd0",
   "metadata": {},
   "source": [
    "The spatial autoregressive coefficient is much larger in absolute value than in the model without WX, but this may be an artifact. The coefficient estimate is not significant. Of the WX terms, only the one for W_Blk14P is significant and the coefficient of Hisp14P is no longer significant. The overall fit of the model barely increases.\n",
    "\n",
    "Of note is that the coefficient for W_Blk14P is negative, whereas the coefficient for Blk14P was positive. This could point to a common factor constraint (i.e., the actual model is a spatial error model), but it does not hold for the other coefficients.\n",
    "\n",
    "Note how the instruments are **W2_W_Blk14P** and **W_W_Blk14P**, etc., so they are the spatial lags of the spatially lagged regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713000f",
   "metadata": {},
   "source": [
    "### SLX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b887df6",
   "metadata": {},
   "source": [
    "For comparison purposes, we also run the Spatial Durbin model without the spatial lag term, i.e., a simple SLX specification. This does not require any special estimation method, but can be accomplished by passing the `slx_lags` argument to the `spreg.OLS` function. We also include the Moran's I test (as well as the other spatial diagnostics) with `spat_diag=True` and `moran=True`.\n",
    "\n",
    "Again, the output listing indicates that the results pertain to an SLX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slx = spreg.OLS(y,x,w=wq,slx_lags=1,\n",
    "                   spat_diag=True,moran=True,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(slx.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2651b6",
   "metadata": {},
   "source": [
    "In contrast to the results for the Spatial Durbin model, in the simple SLX specification all the coefficients of the spatially lagged regressors are significant, but again the coefficient for Hisp14P is not. For Blk_14P and W_Blk_14P, the signs are opposite, suggesting a negative effect of the immediate neighbors, which is difficult to interpret (i.e., it does not make sense for this racial variable, given the high degree of positive spatial autocorrelation for the variable). For HIS_ct and W_HIS_ct, both coefficients are positive and significant, but the value for the spatial lag is *larger* than for the in-situ variable, which violates Tobler's law. Overall, the inclusion of the extra WX terms in the regression specification does not seem to be warranted in this instance.\n",
    "\n",
    "Also, the Moran's I test only weakly suggests the presence of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ffc80",
   "metadata": {},
   "source": [
    "## Spatial Error Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729acfb",
   "metadata": {},
   "source": [
    "As of version 1.4 of `spreg`, all spatial error estimation is implemented through the `GMM_Error` command. This is essentially a wrapper around the original implementations of the various `GM_Error`, `GM_Endog_Error` and `GM_Combo` functions (that still work exactly as before) with a more simplified interface.\n",
    "\n",
    "Beyond the classic spatial error specification, more complex models can be estimated by including `slx_lags` (for additional WX) or `add_wy` (for inclusion of a spatially lagged dependent variable). This yields a range of higher order models, such as SLX Error, SARSAR (spatial lag with spatial errors), and the generalized nested specification (GNS), i.e., a Spatial Durbin model with spatial errors.\n",
    "\n",
    "There are three implementations of the GMM estimation, specified through the `estimator` argument. The default is `estimator='het'`, which accounts for unspecified heteroskedasticity and provides asymptotic standard errors for the spatial autoregressive $\\lambda$ coefficient. Its counterpart that does not account for heteroskedasticity is `estimator='hom'`. Finally, the legacy GM estimator initially suggested by Kelejian and Prucha (1998) and which does not provide any inference for the spatial parameter can be selected with `estimator='kp98'`.\n",
    "\n",
    "Note that because the error variance is modeled explicitly, there are no `robust` options for the standard errors. They are obtained from the FGLS estimation.\n",
    "\n",
    "In what follows, only the default `estimator='het'` case is illustrated, which is the most robust approach in practice.\n",
    "\n",
    "Additional optional arguments that deal with techical details of the estimation are `max_iter` and `step1c` for the `het` case, and `max_iter` and `A1` for the `hom` case. Estimator `kp98` does not have these options. In the examples below, we use the default setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24011fa9",
   "metadata": {},
   "source": [
    "The minimum specification includes `y`, `x` and `w` for the weights as arguments. The output listing includes the specific method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd07c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = spreg.GMM_Error(y,x,w=wq,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(err1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e73932",
   "metadata": {},
   "source": [
    "In contrast to the results for the lag model, here the spatial autoregressive coefficient is highly significant, with p = 0.001. On the other hand, the significance of the Hisp14P coefficient is now p=0.02. The measure of fit is a pseudo R-squared, i.e., the squared correlation between observed and predicted values. Unlike what holds for the spatial lag model, there is only one predicted value, since there is no endogeneity of the spatial term.\n",
    "\n",
    "However, there are two residual vectors. One is the simple residual, the difference $y - X\\beta$, `u`, the other, `e_filtered` is $u - \\lambda W u$, i.e., the residual with the spatial correlation removed. When testing the naive residuals for spatial autocorrelation, one may be surprised to find that it is not controlled for, but that is only the case for the spatially filtered residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b249b",
   "metadata": {},
   "source": [
    "### Exogenous and Endogenous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca901fb",
   "metadata": {},
   "source": [
    "As in the spatial lag case, additional endogenous variables and associated instruments are added by including `yend` and `q` as arguments. Note that the regression matrix is now `xe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c207d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err2 = spreg.GMM_Error(y,xe,w=wq,yend=yend,q=q,\n",
    "                   name_yend=yend_names,name_q=q_names,\n",
    "                   name_y=y_name,name_x=xe_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(err2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5e461",
   "metadata": {},
   "source": [
    "Accounting for endogeneity makes both the spatial autoregressive coefficient and the coefficient of Blk14P insignificant. As mentioned, this is purely to illustrate the mechanics of the process, not a substantive interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d297ef",
   "metadata": {},
   "source": [
    "## SLX Error Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47c808",
   "metadata": {},
   "source": [
    "An SLX Error specification is obtained by including the argument `slx_lags=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799971d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err3 = spreg.GMM_Error(y,x,w=wq,slx_lags=1,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(err3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f201dc7",
   "metadata": {},
   "source": [
    "In this case, only the coefficient of W_HIS_ct is significant, but is is larger than that of HIS_ct, which again violates Tobler's law and is very difficult to interpret. On the other hand, the spatial autoregressive coefficient remains highly significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819edcb",
   "metadata": {},
   "source": [
    "## SARSAR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b4c9f",
   "metadata": {},
   "source": [
    "To estimate the SARSAR model, the additional argument is `add_wy=True` with optionally `w_lags` to specify the order of the spatial lags to be used as the instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97189866",
   "metadata": {},
   "outputs": [],
   "source": [
    "err4 = spreg.GMM_Error(y,x,w=wq,add_wy=True,w_lags=2,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(err4.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247371a",
   "metadata": {},
   "source": [
    "In practice, the SARSAR model often yields counter-intuitive and contradictory results. In this instance, neither of the spatial coefficients is significant and they have opposite signs. On the other hand, the inference on the original regression variables is hardly affected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f62c71",
   "metadata": {},
   "source": [
    "## General 'Nested' Spatial Model - GNSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e255e",
   "metadata": {},
   "source": [
    "Finally, the GNS model is fully loaded, with `slx_lags=1`, `add_wy=True` and optionally `w_lags`. It thus contains both a Spatial Durbin specification and a spatial autoregressive error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "err5 = spreg.GMM_Error(y,x,w=wq,slx_lags=1,add_wy=True,w_lags=2,\n",
    "                   name_y=y_name,name_x=x_names,\n",
    "                   name_w=w_name,name_ds=ds_name)\n",
    "print(err5.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43704aa9",
   "metadata": {},
   "source": [
    "The results are a bit overwhelming and counterintuitive. Both spatial autoregressive coefficients are highly significant (and very close to 1), but of opposite signs. That doesn't really make any sense. Also, again, the coefficient of Hisp14P turns insignificant and the signs of the WX coefficients are all negative. Since a spatial autoregressive error term is already included in the model, the common factor hypothesis is irrelevant. The negative signs are not compatible with the usual interpretation of spatial spillover.\n",
    "\n",
    "In practice, the GNS model is rarely appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9700cd",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "The estimation methods illustrated here have many different options. Explore the sensitivity of the estimation results to some of these options and consider alternative specifications for the\n",
    "base regression model using some additional variables from the data set or for your own model/data.\n",
    "\n",
    "Alternatively, explore the effect of a different spatial weights matrix. Carefully consider the different specifications and what they mean for the interpretation of the *immigrant paradox*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eceb5b",
   "metadata": {},
   "source": [
    "# 5. Spatial Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4dfd7",
   "metadata": {},
   "source": [
    "This section we consider discrete heterogeneity in the form of spatial regimes, where the regimes have been specified exogenously. All the same specifications as before can be implemented in a regimes context, but we will focus only on the main ones: a standard regression (OLS), the spatial lag model with Spatial Durbin option, the spatial error model and an SLX model.\n",
    "\n",
    "Implementation of the regimes estimation is included in a series of `PySAL-spreg` functions ending in `Regimes`, such as `spreg.OLS_Regimes` and `spreg.TSLS_Regimes` for non-spatial models. For spatial models with regimes, both GM and ML estimation is available, respectively in the `GM_Lag_Regimes` and `GMM_Error_Regimes`, as well as in `ML_Lag_Regimes` and `ML_Error_Regimes`.\n",
    "\n",
    "We will also consider endogenous regimes as estimated through the `skater_reg` functionality. Instead of having the regimes specified beforehand, their optimal configuration is obtained by incorporating the least squares functionality in a Skater spatially constrained clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9a9ee",
   "metadata": {},
   "source": [
    "### Setting up the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab8045",
   "metadata": {},
   "source": [
    "We will continue to use\n",
    "four variables from the SDOH data set: **YPLL_rate** (an index measuring premature mortality, i.e., higher values are worse health outcomes), **HIS_ct** (economic hardship index), **Blk14P** (percent Black population), and **Hisp14P** \n",
    "(percent Hispanic population). Note that we can include additional endogenous regressors in the spatial regimes models as well, although we will not consider that here. It works in the same way as for single regressions.\n",
    "\n",
    "As before, we create lists with the variable names for the dependent variable (y_name), the explanatory variables (x_names), the data set (optional, as ds_name), and the weights (optional, as w_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'YPLL_rate'\n",
    "x_names = ['Blk14P','Hisp14P','HIS_ct']\n",
    "ds_name = 'Chi-SDOH'\n",
    "w_name = 'Chi-SDOH_q'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2dbd6",
   "metadata": {},
   "source": [
    "In addition, we need to specify the regimes variable, here **regionno**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvar = 'regionno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs[y_name].values.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26725989",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfs[x_names].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98c842",
   "metadata": {},
   "source": [
    "Currently, the regimes variable has to be a `list`, not a numpy array. We make sure this is the case by means of `tolist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes = dfs[rvar].tolist()\n",
    "type(regimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e311d",
   "metadata": {},
   "source": [
    "## Mapping the Regimes\n",
    "\n",
    "A quick visualization of the spatial layout of the regimes, using the `plot` method for a spatial data frame. For the unique values\n",
    "map depicting the variable `regionno`, we use `categorical=True` as the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.plot('regionno',categorical=True,figsize=(6,6),legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227ea84",
   "metadata": {},
   "source": [
    "## Standard Models\n",
    "\n",
    "### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7b84c",
   "metadata": {},
   "source": [
    "We start with a classic OLS estimation, carried out by means of the `spreg.OLS_Regimes` function. All the arguments are the same as for a standard OLS regression, except for the addition of the `regimes` argument (the third *positional* argument) and its name, as `names_regimes`. In addition, we add a weights matrix and set `spat_diag = True` for spatial diagnostics. We get a listing of the results with the customary `summary` method.\n",
    "\n",
    "The default treatment of the error term is to have a separate variance for each regime, i.e., groupwise heteroskedasticity. This is typically the most reasonable assumption in practice. For a constant error variance across all regimes, the option is `regime_err_sep = False`. We will not consider this here. Since the\n",
    "default is `regime_err_sep = True`, we do not need to specify it in the list of arguments, but it is included here for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = spreg.OLS_Regimes(y, x, regimes, w=wq, spat_diag=True,\n",
    "                        regime_err_sep=True,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(reg1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cf293",
   "metadata": {},
   "source": [
    "The results listing contains a lot of information. For each regime, the coefficient estimates and measures of fit are listed as if they were a separate regression. The order is the order of the regime categories, i.e., the Central, North, South and West regions. This is easy to verify by checking the number of observations listed for each regime. The measures of fit are computed for the subset of observations corresponding to the regime in question. For example, for the smallest subset, the Central region with 28 observations, the adjusted R2 is 0.532. This measure varies considerably across regimes, e.g., with values of 0.304 in North (273 observations), 0.474 for South (334 observations), and 0.709 for West (156 observations). This compares to the overall fit in the homogeneous model of 0.632.\n",
    "\n",
    "The coefficient estimates and their significance vary considerably across regimes, highlighting the underlying spatial heterogeneity. The coefficient of Blk14P is highly significant and positive in the North, South and West, but not at all in the Central region. The coefficient of Hisp14P changes signs, from being negative and marginally significant (at respectively p < 0.04 and p < 0.02) in North and South, to being positive and highly significant in Central, and not significant in the West. Finally, economic hardship is always positive, but only strongly significant in North and South, marginally in Central (p < 0.03) and insignificant in the West (p < 0.07).\n",
    "\n",
    "Each set of regime results also includes the customary regression diagnostics, including diagnostics for spatial effects with `spat_diag = True`, as is the case here. It should be noted that the spatial weights used for these diagnostics are *truncated*, in the sense that each regime becomes an island, without any connections to neighboring observations that are in a different regime. With that caveat in mind, we find little evidence for spatial dependence in the Central, South and West regimes, but strong evidence for lag dependence in the North.\n",
    "\n",
    "Finally, at the bottom of the listing are some overall diagnostics. The Chow test for spatial heterogeneity indicates strong rejection of the null of homogeneity for the Hispanic coefficient, but none for economic hardship and weak evidence for the Blk14P coefficient. The overall test rejects the null at p < 0.02.\n",
    "\n",
    "In addition, there is a global test for spatial effects, i.e., using the full set of residuals and spatial weights for all the observations. There is some weak evidence for an overall spatial lag alternative, but this is not supported by the robust test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27e62e",
   "metadata": {},
   "source": [
    "The individual attributes can be extracted in the usual fashion. An attribute specific to regimes regression is `multi`, a dictionary that contains the regression attributes for each regime separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa301201",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1.multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b1032",
   "metadata": {},
   "source": [
    "Note how reg1.multi is a dictionary, with the regime labels as keys. Hence, to extract the regression information for the first regime, we use the notation multi[1] and not multi[0], since 1 is the key for the first regime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(reg1.multi[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8711070",
   "metadata": {},
   "source": [
    "### SLX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47128ad",
   "metadata": {},
   "source": [
    "The regimes form for the SLX model is simply a special case of the OLS estimation, with the inclusion of `slx_lags=1`. The interpretation is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slx1 = spreg.OLS_Regimes(y, x, regimes, w=wq, spat_diag=True,\n",
    "                        slx_lags=1,\n",
    "                        regime_err_sep=True,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(slx1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d046efc",
   "metadata": {},
   "source": [
    "The inclusion of the additional regressors is not very useful. Only W_HIS_ct is significant in the North and South regions, but none of the other spatially lagged explanatory variables are. The Global Chow tests rejects the null very strongly, at p < 0.01, supported by strong rejection for Hisp14P (p < 0.003) and Blk14P (p < 0.02), but not HIS_ct (p < 0.09). We will not consider this further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf699",
   "metadata": {},
   "source": [
    "## Regime Regression Options\n",
    "\n",
    "The regimes regression has several other options that we do not illustrate here. In addition to the `regime_err_sep = False` already referred to, it is also possible to get heteroskedastic-robust standard errors, with `robust=\"white\"`, or HAC standard errors, which we don't cover in this workshop. In addition, there is an option to implement multiprocessing with the `cores` option (this does not work on all platforms).\n",
    "\n",
    "A more unique option is the possibility to combine fixed coefficients with varying coefficients in so-called hybrid specifications.\n",
    "\n",
    "The combination of fixed and varying coefficients is implemented through the arguments `constant_regi` and `cols2regi`. The first manages the constant term, or intercept, the second deals with the slope coefficients.\n",
    "\n",
    "The default setting for `constant_regi` is `many`, which allows the intercept to vary by regime. The default for `cols2regi` is `all`, which has all the slope coefficients varying by regime.\n",
    "\n",
    "As a result, several combinations are possible, such as:\n",
    "\n",
    "- global constant, varying slope coefficients\n",
    "\n",
    "- global or varying constant, some slope coefficients varying, some not\n",
    "\n",
    "- varying constant, slope coefficient fixed (i.e., fixed effects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd84117",
   "metadata": {},
   "source": [
    "### Spatial Fixed Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa282e1",
   "metadata": {},
   "source": [
    "An alternative specification only lets the constant vary by regimes, as so-called *spatial fixed effects*. This is achieved by setting `constant_regi = 'many'` with the list `colsvari = [False, False, False]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebecf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsvari = [False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = spreg.OLS_Regimes(y, x, regimes, w=wq, spat_diag=True,\n",
    "                        regime_err_sep=True,\n",
    "                        constant_regi='many',cols2regi=colsvari,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(reg2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b7318",
   "metadata": {},
   "source": [
    "The spatial fixed effects coefficients are all highly significant, but not sufficiently different for the Chow test to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd590e2",
   "metadata": {},
   "source": [
    "## Spatial Lag Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521e839",
   "metadata": {},
   "source": [
    "The regimes specification for a spatial lag model can be estimated by means of IV/GMM with `spreg.GM_Lag_Regimes` or by means of maximum likelihood with `spreg.ML_Lag_Regimes`.\n",
    "\n",
    "The arguments are the same as for the standard spatial lag model, with the addition of the `regimes` setting, and the regime-specific options discussed above. In the case of GMM estimation, additional endogenous variables may be included as well, with matching instruments. We do not consider this case here. As before, we set `w_lags=2`.\n",
    "\n",
    "The default (highly recommended) is to have a *global* spatial autoregressive coefficient. If a separate autoregression is desired for each regime, the option `regime_lag_sep` is used. The default is `False`. It should be noted that the spatial weights used when a separate spatial model is specified of each regime are the *truncated* weights. This removes the connectivity structure between observations that belong to different regimes. This is typically not desired in practice, hence the default setting, although there may be situations where it is appropriate (e.g., for block-diagonal weights). We do not consider this further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26aac92",
   "metadata": {},
   "source": [
    "### IV/GMM Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2ea6",
   "metadata": {},
   "source": [
    "GMM estimation is implemented through `spreg.GM_Lag_Regimes`. The arguments are the same as for the\n",
    "standard regression case, with the addition of the spatial weights `w` (and their name) and an option for\n",
    "the number of spatial lags for the instruments, here `w_lags=2`.\n",
    "\n",
    "The same hybrid specifications are allowed as for the standard models by means of the `constant_regi` and `cols2regi` arguments. This is no longer considered separately.\n",
    "\n",
    "In the spatial models, there are two important options pertaining to the error variance and the spatial coefficient. Whereas in OLS estimation, the default is `regime_err_sep = True`, for groupwise heteroskedasticity, this is not the case for the spatial models. The default is to have a single spatial coefficients, i.e., `regime_lag_sep=False`, with `regime_err_sep=False`, but with `robust='white'` for robust standard errors.\n",
    "\n",
    "For clarity, these arguments are specified explicitly, even though they are the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1 = spreg.GM_Lag_Regimes(y, x, regimes, w=wq, w_lags=2,\n",
    "                        regime_lag_sep=False,regime_err_sep=False,robust='white',\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(lag1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8579f",
   "metadata": {},
   "source": [
    "The coefficient listing has the global estimate for the spatial autoregressive coefficient at the bottom. \n",
    "The estimate of 0.251 is not significant. The regimes coefficient estimates are a little different from before: Blk14P is significant and positive in all regions except the first (Central); Hisp14P is only significant (and positive!) in the Central region; while HIS_ct is significant and positive everywhere except in the West.\n",
    "\n",
    "The (spatial) Chow test again shows only weak evidence of heterogeneity, failing to reject the null for HIS_ct, and only weakly rejecting for the Global test (p < 0.05), Blk (p < 0.04) and Hisp14P (p < 0.03)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac851493",
   "metadata": {},
   "source": [
    "#### Separate Spatial Lag Models by Regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d036c92",
   "metadata": {},
   "source": [
    "It is possible to set `regime_lag_sep=True`, but then `regime_err_sep` also needs to be set to `True`, essentially resulting in a separate spatial regression by regime, with truncated spatial weights. If this is not the case, it will be done by the program and a warning will be included in the output listing. The default remains White standard errors, which can be changed by setting `robust=None` for simple groupwise heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fcb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag3 = spreg.GM_Lag_Regimes(y, x, regimes, w=wq, w_lags=2,\n",
    "                        regime_lag_sep=True,robust=None,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(lag3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fd3cb",
   "metadata": {},
   "source": [
    "The separate spatial regressions provide strong evidence to reject the null of homogeneity for all variables except Blk14P. The overall test is also highly significant. The separate spatial autoregressive coefficient is only significant in regime 3. Note that this is based on truncated weights and therefore assumes that each regime is a spatial island, without any interaction with the other regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41927682",
   "metadata": {},
   "source": [
    "## Spatial Durbin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527e573",
   "metadata": {},
   "source": [
    "The regimes specification for a Spatial Durbin model can be obtained as a special case of the spatial lag model with `slx_lags=1` (or higher) included as one of the arguments to either `GM_Lag_Regimes` or `ML_Lag_Regimes`. Since the inclusion of the WX terms in the SLX estimation above did not suggest any spatial effects, we do not pursue this further here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c620d",
   "metadata": {},
   "source": [
    "## Spatial Error Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdd64c",
   "metadata": {},
   "source": [
    "The regimes specification for spatial error models can be estimated by means of both GMM or maximum likelihood. The corresponding commands are `GMM_Error_Regimes` and `ML_Error_Regimes`.\n",
    "\n",
    "The default setting is to again have a single spatial coefficients with `regime_err_sep = False`. The `robust` options are irrelevant since heteroskedasticity can be accounted for in GMM through `estimator='het'` and is not supported in ML estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c732ca",
   "metadata": {},
   "source": [
    "### GMM Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068baf3",
   "metadata": {},
   "source": [
    "The regime-specific arguments to `GMM_Error_Regimes` are the same as above (allowing for hybrid models, etc.) and the other arguments are the same as for `GMM_Error`. The default `estimator='het'` for heteroskedastic-robust standard errors. We only show results for the default case. The other options can be examined in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = spreg.GMM_Error_Regimes(y, x, regimes, w=wq, \n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(err1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc3751",
   "metadata": {},
   "source": [
    "The global spatial autoregressive coefficient is significant with a p-value less than 0.003. The Global Chow test is not significant, and there is only weak indication of heterogeneity for the Hisp14P coefficient (p < 0.03).\n",
    "\n",
    "The other coefficients show the usual pattern. Blk14P is strongly significant and positive in all but the Central region; Hisp14P is only significant in the Central region, but positive; and HIS_ct is significant and positive in all regions, except for the West (p < 0.08)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff4dd2",
   "metadata": {},
   "source": [
    "## Other Error Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af94909",
   "metadata": {},
   "source": [
    "As in the standard case, the SLX-Error model, the SARSAR model and the GNS model can be obtained by passing the proper combination of `slx_lags=1` and `add_wy=True` arguments to the `GMM_Error_Regimes` command. We do not further consider this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c019a6",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "The functions considered here have many options. Assess the effect of these\n",
    "options on the results and interpretation. For example, allow some regressors to vary while\n",
    "keeping others fixed, check the impact of heteroskedasticity on the standard errors,\n",
    "and/or vary the spatial weights. Which of the various specifications seems to make the\n",
    "most sense in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b039d48",
   "metadata": {},
   "source": [
    "## Endogenous Regimes - Skater Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f73616",
   "metadata": {},
   "source": [
    "The Skater-regimes approach proceeds in two steps. In the first, the optimal regimes allocation is obtained for a given number of regimes. More precisely, the optimization is carried out for all number of regimes up to the specified value of k.\n",
    "\n",
    "In the second step, a *hard allocation* is applied to obtain the final regression coefficients. Even though regressions are used in the Skater optimization process, they are not available in the resulting Skater object.\n",
    "\n",
    "To facilitate the optimization, the matrix of explanatory variables is first standardized. This is used to compute an initial *distance* matrix (in attribute space) from which the minimum spanning tree is obtained. The remainder of the algorithm is based on the regression results using the original (unstandardized) x matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = (x - np.mean(x,axis=0)) / np.std(x,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457d94e",
   "metadata": {},
   "source": [
    "### Optimization Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411f5d8",
   "metadata": {},
   "source": [
    "The command to compute the Skater regimes (regions) is `spreg.skater_reg.Skater_reg`, followed by a `fit` method. This follows Python convention (e.g., in scikit-learn) where first an object is created (or a class), to which then specific methods can be applied or attributes extracted. In practice, the two steps are combined, as illustrated below. The particular structure follows from the implementation of the Skater algorithm in PySAL's `spopt` package, from which skater-reg is derived.\n",
    "\n",
    "The arguments to the `fit` method are the number of regimes (here 4), followed by the spatial weights and the matrix used to compute the initial distance matrix, here `x_std`. The fourth argument is a dictionary, with keys `reg`, `y`, and `x`, respectively for the type of regression (both OLS and GM_Lag are currently supported), the dependent variable and the matrix of explanatory variables (a constant term is assumed and should not be included). Note that the matrix of explanatory variables is in the original scale, and is *not* the standardized matrix used for the distance calculations. A final argument is the minimum size for each region, i.e., the minimum number of observations required for regime estimation, as `quorum`. In our example, `quorum = 100`. \n",
    "\n",
    "Depending on the total number of observations and the maximum number of regimes considered, the quorum may lead to the lack of a solution. Because of the nature of the Skater algorithm, any subsequent *cut* in the minimum spanning tree leads to a smaller subregion. When such a subregion fails to meet the `quorum` minimum size, the algorithm stops and fails to reach the required number of regimes.\n",
    "\n",
    "In our example, we create the skater object `endr1`. Note that this class (currently) does not have a `summary` method. The command `spreg.skater_reg.Skater_reg( )` initializes the skater class. The method `fit` computes the regimes allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "endr1 = spreg.skater_reg.Skater_reg().fit(4, wq, x_std, {'reg':spreg.OLS,'y':y,'x':x}, quorum=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806138b",
   "metadata": {},
   "source": [
    "The resulting object is very complex, as illustrated by a `dir` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45353",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dir(endr1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b52b",
   "metadata": {},
   "source": [
    "For our purposes, the most important attribute is the `_trace`, a list of lists which contains for each *cut* a vector (numpy array) with the resulting region labels and the information on in and out node, as well as the value for the overall objective function. The list contains k=4 elements, but the first one is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de265b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(endr1._trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endr1._trace[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f6a06",
   "metadata": {},
   "source": [
    "The full list has the results for k=2, 3 and 4 in positions 1, 2, and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1149ce0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "endr1._trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021351b2",
   "metadata": {},
   "source": [
    "For k=4, the results are in position 3. The first item is a numpy array with the regime labels, going from 0 to 3. Listing the first few items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endr1._trace[3][0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d9c3c",
   "metadata": {},
   "source": [
    "The second element of the list in position 3 is itself a list with the `in_node`, the `out_node`, and, more importantly, the value for the objective function, the `score`. We extract the score as item `[3][1][2]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafa68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endr1._trace[3][1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a76ea",
   "metadata": {},
   "source": [
    "## Hard Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4465f",
   "metadata": {},
   "source": [
    "The first Skater optimization step is followed by a standard regimes regression using the solution as a *hard allocation* to regimes.\n",
    "\n",
    "The final regime classification obtained by the Skater algorithm is contained in the vector in position `[3][0]`. In order to keep consistency with the regime labels used before, we add 1 to the classification, so that it now ranges from 1 to 4. We store the result in the `regimes` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes = endr1._trace[3][0] + 1\n",
    "regimes[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548885c",
   "metadata": {},
   "source": [
    "### Regimes plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed596",
   "metadata": {},
   "source": [
    "In order to make a map of the regimes, we need to add a new variable to the spatial data frame `dfs`. We call the variable `sk_regions` and use the `plot` method to create a simple unique values map, with the regime regions labeled 1 through 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"sk_regions\"] = regimes\n",
    "dfs.plot('sk_regions', categorical=True, figsize= (7,7), legend=True, cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebe054",
   "metadata": {},
   "source": [
    "### Regime Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd4058",
   "metadata": {},
   "source": [
    "We can now also carry out the regimes regression using `OLS_Regimes` with the `regimes` result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db00e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = spreg.OLS_Regimes(y, x, regimes, w=wq, spat_diag=True,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(reg1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce9a0e",
   "metadata": {},
   "source": [
    "The results are quite different from the exogenous regimes exercise we carried out previously. While the Global test strongly rejects the null of homogeneity (p < 0.0000), for the individual coefficients, this is only the case for `HIS_ct`. The latter is positive and highly significant in regimes 2 and 3, but not in 1 and 4. `Blk14P` is highly significant and positive in all regimes, but without sufficient evidence to reject the null of homogeneity. `Hisp14P` is not significant in any of the regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18a834",
   "metadata": {},
   "source": [
    "## Optimal Number of Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829cd0c",
   "metadata": {},
   "source": [
    "So far, we have used the same number of regimes as in the exogenous case. However, `Skater_reg` computes all the allocations up to the given value of k, which allows for the construction of a so-called *elbow function*. The latter shows the evolution of the overall measure of fit with a change in number of regimes. As the number of regimes increases, the fit is always better, but at some point, the improvement is no longer substantial, suggesting a *break* in the plot. This indicates the optimal number of regimes. In practice, detecting a break is not always that easy, as illustrated below.\n",
    "\n",
    "We will now run the Skater algorithm for all regime allocations up to k=15. In order to obtain feasible solutions, we have to lower the `quorum` to 30. For a higher number of regimes, the quorum has to be made even smaller, which does no longer allow for meaningful regression results in the smallest regimes.\n",
    "\n",
    "The command is the same as before, but now with `15` as the number of regimes, and `quorum=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3091059",
   "metadata": {},
   "outputs": [],
   "source": [
    "endropt = spreg.skater_reg.Skater_reg().fit(15, wq, x_std, {'reg':spreg.OLS,'y':y,'x':x}, quorum=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759434b",
   "metadata": {},
   "source": [
    "We can now extract the value for the objective function, the score from each item in the `_trace` attribute. We create a list with those values as trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = [endropt._trace[i][1][2] for i in range(1,len(endropt._trace))]\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e737426",
   "metadata": {},
   "source": [
    "The first value is the score for k=2, next for k=3, etc. We see how the value *decreases* from 2,780,615,032 to 2,251,989,467 for k=15. This is visualized by means of an elbow plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a34cd",
   "metadata": {},
   "source": [
    "### Elbow Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb45165",
   "metadata": {},
   "source": [
    "The elbow plot is a simple line graph of the value of the score against k. We create a list for k to be used for the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad053f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvalues = np.arange(2,len(endropt._trace)+1)\n",
    "kvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf009a",
   "metadata": {},
   "source": [
    "We now use the seaborn `sns.lineplot` function to graph the sum of squared residuals on the number of regimes. First, we use `set_themes` to have a nice grid background (totally optional, of course). The `lineplot` command itself, just requires the `x` and `y` arguments. Since these are lists in our example, we do not need to specify a data frame. To spiff up the graph, we place dots at the actual values and set labels for the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22102fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (pd.DataFrame({'Number of Regimes': kvalues, 'Sum of Squared Residuals': trace})\n",
    "      .plot(x='Number of Regimes', y='Sum of Squared Residuals', marker='o'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfec19f",
   "metadata": {},
   "source": [
    "The result suggests a possible break at 7. Consequently, we extract the regime classification from position `6`. As before, we add 1 to the result to retain compatible labels. The result is stored in the new regimes variable `regimes1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes1 = endropt._trace[6][0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b68110",
   "metadata": {},
   "source": [
    "### Optimal Regimes Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf77ff9",
   "metadata": {},
   "source": [
    "We can now add the new regimes variable to our spatial data frame as `opt_regions` and use the `plot` method to obtain a unique values map. We reset the `style` to `white` to get rid of the grid lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"opt_regions\"] = regimes1\n",
    "dfs.plot('opt_regions', categorical=True, figsize=(7,7),legend=True, cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb804c9",
   "metadata": {},
   "source": [
    "### Optimal Regimes Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137f524",
   "metadata": {},
   "source": [
    "Finally, we obtain the regimes regression using `OLS_Regimes` with `regimes1` as the new regimes variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regopt = spreg.OLS_Regimes(y, x, regimes1, w=wq, spat_diag=True,\n",
    "                        name_y=y_name, name_x=x_names, name_regimes=rvar,\n",
    "                        name_w=w_name,name_ds=ds_name)\n",
    "print(regopt.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c4565",
   "metadata": {},
   "source": [
    "The size of the resulting regimes varies from 33 for regime 3 to 210 for regimes 1 and 5. The regimes diagnostics provide strong evidence for heterogeneity. The Global test is significant at p = 0.0000, and the null is strongly rejected for both `Blk14P` (p < 0.0006) and `Hisp14P` (p < 0.0042), and weakly so for `HIS_ct` (p < 0.0273). The individual regression results vary considerably as well, with adjusted R2 ranging from a low 0.0284 in regime 4, to 0.6193 in regime 3. A further substantive interpretation of the coefficient variation is beyond our scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655cb32b",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692c515",
   "metadata": {},
   "source": [
    "Consider the optimal regimes allocation for different spatial weights, i.e., other than queen contiguity. Also assess the extent to which the results are affected by including the dependent variable in the calculation of the original distance matrix (i.e., not only standardized x, but also standardized y), a practice that is sometimes employed to find data-driven spatial regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a20b0e",
   "metadata": {},
   "source": [
    "# 6. Spatial Panels Using SUR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdafd12",
   "metadata": {},
   "source": [
    "Up to this point, we have considered only estimators and tests limited to cross-sectional analysis. In this section we consider specifications for which observations are available in two dimensions: space and time. The Seemingly Unrelated Regression (SUR) models take into account a limited degree of simultaneity in the form of dependence between the error in different equations. As stated in Anselin (1988), if the equations pertain to time series for different regions, the resulting dependence can be considered as a form of spatial autocorrelation. \n",
    "\n",
    "Most of the same specifications as before can be implemented in a SUR context, but we will focus only on the main ones: a standard regression (OLS), the spatial lag model, and the spatial error model. \n",
    "\n",
    "Implementation of the SUR models is included in a series of `PySAL-spreg` functions, such as `spreg.SUR` and `spreg.ThreeSLS` for non-spatial models. For spatial models, both GM and ML estimation is available, in the `SURlagIV`, as well as in `SUR_errorGM` and `SUR_errorML`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aab6f5",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "We will be using data on homicides and selected socio-economic characteristics for continental U.S. counties. Data for four decennial census years: 1960, 1970, 1980 and 1990:\n",
    "\n",
    "- **NAT.shp,shx,dbf,prj**: homicides and selected socio-economic characteristics for 3,085 continental U.S. counties\n",
    "\n",
    "You can find more info on the data here: https://geodacenter.github.io/data-and-lab/ncovr/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data/NAT/\"\n",
    "infileshp = folder+\"NAT.shp\"   # input shape file\n",
    "outfileshp = folder+\"out_NAT.shp\" # output shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf49ac",
   "metadata": {},
   "source": [
    "We will read the shapefile the same way as before, using the function `read_file`, followed by the file pathname in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611493f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = gpd.read_file(infileshp)\n",
    "print(dfs.shape)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a2bcb",
   "metadata": {},
   "source": [
    "Since there are too many observations in the data (think 4 times periods * 3,085 counties), we will select only the counties in southern states to simplify our applications on this workshop. We will also select only the variables we are interested in. Feel free to run the examples using the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2 = dfs[dfs['SOUTH'] == 1]\n",
    "dfs2 = dfs2[['FIPSNO','HR80','HR90','PS80','PS90','UE80','UE90','RD80','RD90','geometry']]\n",
    "print(dfs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf232e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2.plot('HR90', figsize=(6, 6),cmap='OrRd',scheme='quantiles',k=4,legend=True,\n",
    "       legend_kwds={'loc':'center left','bbox_to_anchor':(1,0.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5a335",
   "metadata": {},
   "source": [
    "The resulting shapefile can be exported using `Geopandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2.to_file(outfileshp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165520e",
   "metadata": {},
   "source": [
    "### Queen Contiguity Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63310f4",
   "metadata": {},
   "source": [
    "The weights can be constructed the same way as before using `libpysal` package in PySAL. The function is `weights.Queen.from_dataframe` with as arguments the geodataframe and optionally the `ids` (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1 = libpysal.weights.Queen.from_dataframe(dfs2,ids='FIPSNO')\n",
    "wq1.transform = 'r'\n",
    "wq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51db469",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq1.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1174e",
   "metadata": {},
   "source": [
    "### Setting up the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7011e8a",
   "metadata": {},
   "source": [
    "The specification of the model to be estimated can be provided as lists. Each equation should be listed separately. In this example, equation 1 has HR80 as dependent variable and PS80 and UE80 as exogenous regressors. For equation 2, HR90 is the dependent variable, and PS90 and UE90 are the exogenous regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = ['HR80','HR90']\n",
    "x_var = [['RD80','PS80','UE80'],['RD90','PS90','UE90']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273a3e7",
   "metadata": {},
   "source": [
    "The SUR method requires data to be provided as dictionaries. `PySAL-spreg` provides the tool `sur_dictxy` to create these dictionaries from the list of variables. The line below will create four dictionaries containing respectively the dependent variables (bigy), the regressors (bigX), the dependent variables' names (bigyvars) and regressors' names (bigXvars). \n",
    "\n",
    "The data should be read using `PySAL`'s io format to match the format expected by `sur_dictxy`, as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pysal = libpysal.io.open(folder+\"out_NAT.dbf\",'r')\n",
    "bigy,bigX,bigyvars,bigXvars = spreg.sur_dictxy(db_pysal,y_var,x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f11e8",
   "metadata": {},
   "source": [
    "The SUR format stores each equation separatly in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigy.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28239c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y:\", bigy[0].shape, \"X:\", bigX[0].shape, \"Y_name:\",bigyvars[0], \"X_names:\",bigXvars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a565cb",
   "metadata": {},
   "source": [
    "## Non-spatial models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e56426",
   "metadata": {},
   "source": [
    "For exogenous regressors only, we can use the function `SUR` to estimate the model and assess spatial diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = spreg.SUR(bigy,bigX,w=wq1,name_bigy=bigyvars,name_bigX=bigXvars,\n",
    "                 spat_diag=True,name_ds=\"out_NAT.dbf\",name_w=\"Queen_1\")\n",
    "print(reg1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a384fb",
   "metadata": {},
   "source": [
    "The main interest in the classic spatial SUR model is to assess the extent to which the crossequation covariances (or, equivalently, correlations) are signiﬁcantly diﬀerent from zero. If that is not the case, then there is no gain in eﬃciency from a SUR approach over separate equation-by-equation regressions. The two most commonly used tests in this context are based on the Likelihood Ratio (LR) and the Lagrange Multiplier (LM) principle (for technical details, see again, e.g., Greene, 2012). In this example, both test statistics are significant, suggesting that the SUR approach is more efficient than separate estimations.\n",
    "\n",
    "From the Chow test between equations, we can see that the coefficients for the different time periods are significantly different for the variables RD and UE, with p-values < 1%. Only the coefficients for UE and the constants are not significantly different across equations.\n",
    "\n",
    "The spatial diagnostics reject the null hypothesis of spatial randomness for both error and lag alternatives. Since both statistics have similar values, we can run the two different specifications and compare the models’ fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f054e9b",
   "metadata": {},
   "source": [
    "## Spatial SUR models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330276e",
   "metadata": {},
   "source": [
    "### Spatial Lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3544b2",
   "metadata": {},
   "source": [
    "Let's start with a spatial lag specification since the omission of a relevant spatial lag would render the estimated coefficients inconsistent. We can use the function `SURlagIV`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7363c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lag = spreg.SURlagIV(bigy,bigX,w=wq1,name_bigy=bigyvars,name_bigX=bigXvars,\n",
    "                               name_ds=\"NAT\",name_w=\"Queen_1\")\n",
    "print(reg_lag.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448983d8",
   "metadata": {},
   "source": [
    "The results from this model suggest that the spatial lag is significant only for the data from the year 1990, since the p-value for W_HR90 is smaller than 5%. For 1980, the z-statistic of the spatial lag does not reject the null hypothesis. Keep in mind that albeit not significant for the first equation, the spatial lag remains as part of the specification, so it should either be removed from that equation or the coefficients must be interpreted considering the spatial multiplier effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7251c",
   "metadata": {},
   "source": [
    "Alternatively, to estimate a spatial error model, we can either use the function `SURerrorGM` or `SURerrorML`. Note that neither of these functions provides inference on the spatial autoregressive coefficient `lambda`, which can be simulated using an empirical distribution and bootstrapping techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7326b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_error = spreg.SURerrorGM(bigy,bigX,w=wq1,name_bigy=bigyvars,name_bigX=bigXvars,\n",
    "                               name_ds=\"NAT\",name_w=\"Queen_1\")\n",
    "print(reg_error.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbccf1",
   "metadata": {},
   "source": [
    "# 7. Probit Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8de6bb",
   "metadata": {},
   "source": [
    "Up to this point, we have considered only estimators and tests for continuous dependent variables. In this section  we turn to diagnostic test for spatial autocorrelation in the probit model.\n",
    "\n",
    "This functionality is contained in the `spreg.Probit` class. There are other ways to estimate probit models in Python, such as in the `statsmodels` module, but these implementations do not include the spatial diagnostics.\n",
    "\n",
    "For the sake of convenience, the estimation of a probit model as well as the diagnostics are both included.\n",
    "Specifically, the probit test by Pinkse and Slade (1998), Pinkse (1998), and Kelejian and Prucha (2001) are implemented. These are all based on slightly different forms of generalized residuals.\n",
    "\n",
    "The data set used is the so-called **Katrina** data set, used as an empirical illustration in a paper\n",
    "by LeSage et al (2011). This is included as a sample data set in the `R` `ProbitSpatial` and `spatialprobit` packages. The original data have been edited to add UTM Zone 15N projected coordinates (in addition to the original lat-lon), a unique identifier (ID), and a constant term (const). It is available as both a shape file and a csv file. We will use the latter. The variable names have been slightly adjusted to work as a shape file in `GeoDa`.\n",
    "\n",
    "Note that the Katrina data set consists of three separate near-linear street segments. This will cause some problems with contiguity-based spatial weights. Therefore, nearest-neighbor weights are used in this illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8163dde",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "We will use the Katrina data file and two (symmetric) k-nearest neighbor weights files created in `GeoDa` with k=11 (the one used by LeSage et al 2011). The following files should be in the working directory:\n",
    "\n",
    "- **Katrina_utm.csv**: data on store reopenings after hurricane Katrina on three street segments in New Orleans\n",
    "- **Katrina_utm_k11s.gal**: symmetric k-nearest neighbor weights for k=11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9514b2",
   "metadata": {},
   "source": [
    "## Read in Data and Weights Files\n",
    "\n",
    "The data are read in from the **Katrina_utm.csv** file using the pandas `read_csv` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data/Katrina/\"\n",
    "df = pd.read_csv(folder+\"Katrina_utm.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b315e9",
   "metadata": {},
   "source": [
    "The two weights are read from the gal file and row-standardized. Finally, a quick check on the contents. Ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk11 = libpysal.io.open(folder+\"Katrina_utm_k11s.gal\").read()\n",
    "wk11.transform = 'r'\n",
    "wk11.n\n",
    "wk11.weights['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561a244",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "The basic model specification uses the same variables as in LeSage et al (2011). The dependent variable is `y1`, whether or not a store has re-opened within three months of the hurricane. The explanatory variables are:\n",
    "- `flood_dpth`: depth of flooding\n",
    "- `log_medinc`: log of median income\n",
    "- `small_size`: dummy for small size (medium is reference)\n",
    "- `large_size`: dummy for large size (medium is reference)\n",
    "- `low_cust`: low status customers dummy\n",
    "- `high_cust`: high status customers dummy\n",
    "- `sole_prop`: sole proprietorship dummy\n",
    "- `nat_chain`: national chain dummy\n",
    "\n",
    "As in the previous notebooks, we first create lists with the variable names for the dependent variable (y_name), the explanatory variables (x_names), the data set (optional, as ds_name) and the weights (optional, as w_name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'y1'\n",
    "x_names = ['flood_dpth','log_medinc','small_size','large_size','low_cust','high_cust',\n",
    "           'sole_prop','nat_chain']\n",
    "ds_name = 'Katrina_utm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd4381",
   "metadata": {},
   "source": [
    "In *spreg*, the dependent and explanatory variables need to be passed, respectively as a numpy matrix of dimension nx1 and as a numpy matrix of dimension nxk (with k as the number of explanatory variables, not including the constant term).\n",
    "\n",
    "This is a two-step process: first the variables are pulled from the pandas data frame using the bracket notation, subsequently this data frame is turned into a numpy array. We carry out a quick check on the dimension using `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e80e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[y_name].values.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[x_names].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982234c",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We start with a linear regression, just as a quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = spreg.OLS(y,x,name_y=y_name,name_x=x_names,name_ds=ds_name)\n",
    "print(reg1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15a6f2",
   "metadata": {},
   "source": [
    "The fit is rather poor (not a surprise, since it is a linear approximation to a non-linear function),\n",
    "with an adjusted $R^2$ of 0.28. The `flood_dpth` variable is strongly significant (p=0.0000) and negative, whereas\n",
    "`log_medinc` is positive and strongly significant (p=0.000001). In addition low status customers are also significant and\n",
    "negative (p=0.003), and sole proprietorship is slightly significant and positive (p=0.011). The other variables are not significant in a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deaecdc",
   "metadata": {},
   "source": [
    "## Probit Regression\n",
    "\n",
    "First, we run the probit regression without diagnostics. The arguments are largely the same as for OLS regression, except that there is also an argument `optim` (default is `newton`, alternatives are `ncg` and `bfgs` -- see the docs for details), and `scalem`, the method to calculate the scale of the marginal effects (default is `phimean`).\n",
    "\n",
    "For our example, this gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb1 = spreg.Probit(y,x,optim='bfgs',w=wk11,spat_diag=True,\n",
    "                   name_y=y_name,name_x=x_names,name_ds=ds_name)\n",
    "print(pb1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95295140",
   "metadata": {},
   "source": [
    "In addition to the four significant variables identified in the linear regression, the probit regression now also includes `small_size` as weakly significant (p=0.05) and negative. The signs of the significant variables are the same as in the linear regression, but now sole propietorship is significant at p=0.004. The output also lists some measures of fit, which are not relevant in our current context (they would be if we compared different probit specifications)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cc8fd",
   "metadata": {},
   "source": [
    "All three spatial diagnostics strongly reject the null of no error spatial autocorrelation. Note that the result for the Kelejian-Prucha test is the standard normal form, so 2.896 is the value of a z-statistic, not a Chi-squared statistic as for the other two diagnostics. Overall, there is rather strong evidence that including the explanatory variables has not removed\n",
    "the spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fee17",
   "metadata": {},
   "source": [
    "`Spreg` does not provide functions to estimate spatial lag or spatial error Probit models, but we can reassess the spatial dependence in a SLX specification by adding spatial lags of the regressors. For the Probit function, these lags must be calculated before the function is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slx = libpysal.weights.lag_spatial(wk11, x)\n",
    "x_slx = np.hstack((x,slx))\n",
    "x_names2 = x_names + ['W_' + item for item in x_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eec5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_slx = spreg.Probit(y,x_slx,optim='bfgs',w=wk11,spat_diag=True,\n",
    "                   name_y=y_name,name_x=x_names2,name_ds=ds_name)\n",
    "print(pb_slx.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e3d81",
   "metadata": {},
   "source": [
    "From the results above, we can see that including the spatial lagged regressors resulted in no remaining spatial autocorrelation in the residuals. Hence, the classic spatial probit estimator with spatial lagged regressors is the correct estimator for this specification and there is no need to estimate a spatial lag probit model or spatial error probit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4d64b",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Consider the extent of spatial autocorrelation using the other two dependent variables, `y2` (re-opening after 6 months), and `y3` (re-opening after 12 months). Alternatively, create additional spatial weights for different numbers of nearest neighbors (make sure the weights are symmetric) and assess the sensitivity of the result to different neighbor specifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
